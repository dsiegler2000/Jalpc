---
layout: post
title:  "My Summer at Cleveland State University 2018"
date:   2019-09-11
desc: "This post describes my time during the summer of 2018 at Cleveland State University"
keywords: "blog,ml,python,research"
categories: [Python, Machine Learning, Other Work]
tags: [python,machine learning,research]
icon: icon-python
---
During the later half of the summer of 2018, I worked in the Internet of Things lab of Dr. Wenbing Zhao at Cleveland State University (CSU). During a portion of the summer, I was able to work at both WRIS and CSU part time, as both allowed me to work from home (or very close to home) easily. Towards the end of the summer, I worked at CSU full time.

When I first started, a friend and I were tasked with learning how to develop for the Microsoft HoloLens. Luckily, Microsoft published a plugin for the HoloLens for Unity, which made the whole process significantly easier as we both knew C# and Unity decently well. However, the build and deploy process presented us with issues throughout our time there. We were constantly dealing with build errors and coming up with clever debugging tools, as there were none available for the HoloLens, and the plugin wasn't very well documented.

Once we learned how to write for the HoloLens, we were then tasked with writing nurse training scenarios using the HoloLens. We first worked on a general technique for these scenarios, then applied it to the scenario of giving a shot.

The HoloLens has quite advanced tracking and mapping features. It has four cameras, an IMU, a depth sensor, and a light sensor. Notably, the HoloLens lacks any eye tracking hardware. The software built by Microsoft performs a version of simultaneous mapping and localization (SLAM). This results in a mesh of the surfaces in the environment, as well as your location in that environment.  

![mesh](/static/files/csu20181.png)
**_This is an example of the spacial mapping computed by the HoloLens. Obviously it is quite noisy, but it is still quite impressive._**

We made use of primarily the camera on the HoloLens, although at one point we experimented with using the mesh to find flat surfaces.

For general purpose scenarios, we came up with the idea of using QR-style vision markers to track certain objects and track if the person is looking at something. QR codes are designed to store large amounts of information and be readable from as wide a variety of angles as possible. Vision markers, on the other hand, might only encode a byte of data (a number 0-255), but they are designed so that an algorithm can quickly and easily determine the rotation and translation of the marker relative to the camera.

![aruco marker](/static/files/csu20182.png)
**_This is an example of vision markers. The camera can track the position and the orientation of the markers (indicated by the pink squares and the red green and blue axes)._**

Creating custom vision markers is a very difficult challenge. Thus, we decided to use a pre-existing vision marker library. The state-of-the-art, especially for robotics applications, is [ArUco](https://www.uco.es/investiga/grupos/ava/node/26). We had quite a bit of trouble trying to build this for Unity, mostly because OpenCV, a package ArUco relies on, has poor bindings for Unity. Thus, we decided to go with [Vuforia VuMarks](https://library.vuforia.com/articles/Training/VuMark), which have the same effect.

Once we implemented these Vuforia markers in Unity and added a few extra functions to integrate in the HoloLens, we had a powerful system for building scenarios. This system has two main functions. First, there is gaze tracking. The HoloLens doesn't have eye tracking, but using the markers we could see if the user's head was pointed at a specific place by raycasting the HoloLens position and seeing if it is near a VuMark (using the mesh generated by the HoloLens to make this a bit more precise). Second, we could track static objects in the world stably. By placing VuMarks at known locations on an object, we could determine that object's position in space very well simply by averaging these positions (taking into account the relative placement of the VuMarks on the object).

Using this system, we made a small scenario for giving shots. We placed markers under each "needle" so we could track if the nurse selected the correct needle and whether they were hesitating. We also placed markers on a medical mannequin. This allowed us to overlay a hologram of the anatomy on the mannequin.

![mannequin](/static/files/csu20183.png)
**_This is what the anatomy hologram overlay looks like._**

A portion of our work was presented at the [IEEE Smart World Congress](http://www.smart-world.org/2019/) and will be published online shortly.  
